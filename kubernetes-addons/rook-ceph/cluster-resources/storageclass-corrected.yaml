# storageclass-corrected.yaml
# kubectl apply -f storageclass-corrected.yaml 명령어로 배포합니다.
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  # 복제본 기반 풀 설정
  replicated:
    # 3개의 복제본을 유지하여 데이터 안정성을 높입니다. (최소 3개의 노드가 필요)
    size: 3
    # OSD에 문제가 생겼을 때 복제본 수를 일시적으로 줄이는 것을 허용
    requireSafeReplicaSize: true
  # CRUSH 맵에서 장애 도메인을 'host'로 설정하여 각 복제본이 다른 노드에 저장되도록 합니다.
  failureDomain: host
  # 이 풀을 블록 스토리지(RBD)용으로 활성화합니다.
  enableRBDStats: true
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
  annotations:
    # 이 스토리지 클래스를 기본값으로 사용하려면 아래 주석을 해제하세요.
    # storageclass.kubernetes.io/is-default-class: "true"
provisioner: rook-ceph.rbd.csi.ceph.com # RBD CSI 프로비저너
parameters:
  # CephBlockPool의 이름과 CephCluster의 네임스페이스를 지정합니다.
  pool: replicapool
  clusterID: rook-ceph # CephCluster가 위치한 네임스페이스
  
  # imageFeatures는 layering, exclusive-lock, object-map, fast-diff, deep-flatten 등을 지원합니다.
  # Kubernetes v1.25+ 환경에서는 layering,exclusive-lock 사용이 권장됩니다.
  imageFeatures: layering,exclusive-lock
  
  # CSI 드라이버가 생성할 파일 시스템 타입을 지정합니다.
  csi.storage.k8s.io/fstype: xfs
  
  # 생성된 PV에 대한 Secret 이름을 지정합니다.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

# PV의 reclaim 정책. Retain은 PV가 삭제되어도 실제 데이터는 보존됩니다.
# 개발/테스트 환경에서는 Delete로 설정하여 PVC 삭제 시 PV와 데이터도 함께 삭제할 수 있습니다.
reclaimPolicy: Retain
allowVolumeExpansion: true
mountOptions:
  - discard
